{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Vocabulary used in Control vs. Activity Video Conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lexical_diversity import mtld, hdd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "from statistics import mean\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1cond = pd.read_csv(\"exp1_lexical_diversity.csv\")\n",
    "e2cond = pd.read_csv(\"exp2_lexical_diversity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a function to clean text in brackets\n",
    "def clean(string):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    for i in string:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif skip1c == 0:\n",
    "            ret += i\n",
    "    return ret\n",
    "\n",
    "def find_id(conds, fname):\n",
    "    for i in range(len(conds[\"sid\"])):\n",
    "        val = fname.find(conds[\"sid\"][i])\n",
    "        if(val!=-1):\n",
    "            return(conds[\"sid\"][i])\n",
    "    return(\"NA\")\n",
    "\n",
    "def type_token_count(text):\n",
    "    # remove all punctuations\n",
    "    text_ready = list(text)\n",
    "    n_words = len(text_ready)\n",
    "    for i in range(n_words):\n",
    "        for c in string.punctuation:\n",
    "            text_ready[i] = text_ready[i].replace(c,'')\n",
    "    # remove empty words\n",
    "    text_ready = list(filter(None, text_ready))\n",
    "    text_ready = ''.join(text_ready)\n",
    "    token_value = len(str.split(text_ready))\n",
    "    word_list = nltk.word_tokenize(text_ready)\n",
    "    #lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    #print(lemmatized_output)\n",
    "    #type_value = len(set(str.split(lemmatized_output)))\n",
    "    type_value = len(set(word_list))\n",
    "    return type_value, token_value\n",
    "\n",
    "# formatted mother utterances (each utterance should be in the same one paragraph) by hand\n",
    "# \"Female speaker\" \"*Other Child\" \"*OTH\" \"*BOTH\" \"*ADF,ADM\" refer to speakers other than parent/child\n",
    "# should replace all \"CHI 2:\" with \"CHI:\"\n",
    "def load_transcripts(folder, conds):\n",
    "    col_names = ['sid', 'MTLD', 'HDD', 'types', 'tokens', 'TTR', 'len_sentence']\n",
    "    results = pd.DataFrame(columns = col_names)\n",
    "    df = pd.DataFrame(columns=['sid','cond','text'])\n",
    "    df['cond'] = \"\"\n",
    "    conds[\"types2\"] = np.nan\n",
    "    conds[\"tokens2\"] = np.nan\n",
    "    conds[\"MTLD2\"] = np.nan\n",
    "    conds[\"TTR2\"] = np.nan\n",
    "    files = os.listdir(folder)\n",
    "    try:\n",
    "        files.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    docs = {} # for spacy analysis\n",
    "    # print (files)\n",
    "    for fname in files:\n",
    "        name =  fname\n",
    "        path = \"\".join((folder,name))\n",
    "        fr = open(path, 'r')\n",
    "        lines = []\n",
    "        for line in fr:\n",
    "            spm = line.split('*MOT:')\n",
    "            spm_1 = line.split('MOT:')\n",
    "            spd = line.split('*FAT:')\n",
    "            spd_1 = line.split('FAT:')\n",
    "            if len(spm)>1:\n",
    "                lines.append(spm[1]) # mother speech\n",
    "            elif len(spd)>1:\n",
    "                lines.append(spd[1]) # father speech\n",
    "            elif len(spd_1)>1:\n",
    "                    lines.append(spd_1[1])\n",
    "            elif len(spm_1)>1:\n",
    "                lines.append(spm_1[1])\n",
    "        lines_str = \" \".join(lines)\n",
    "        lines_str = lines_str.replace ('\\n', '')\n",
    "        lines_str = lines_str.replace ('\\t', '')\n",
    "        text_ready = clean(lines_str)\n",
    "        \n",
    "        type_value, token_value = type_token_count(text_ready)\n",
    "        mtld_value = mtld(text_ready.split())\n",
    "        \n",
    "        id = find_id(conds, fname)\n",
    "        #print(id)\n",
    "        #print(conds.loc[conds['sid'] == id])\n",
    "        conds.loc[conds['sid'] == id, 'text'] = text_ready\n",
    "        conds.loc[conds['sid'] == id, 'types2'] = type_value\n",
    "        conds.loc[conds['sid'] == id, 'tokens2'] = token_value\n",
    "        conds.loc[conds['sid'] == id, 'MTLD2'] = mtld_value\n",
    "        conds.loc[conds['sid'] == id, 'TTR2'] = type_value / token_value\n",
    "        #docs[id] = nlp(text_ready) # spacy container for document and all annotations\n",
    "    return conds\n",
    "\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f6d6a251ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me1docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_transcripts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exp1_trans_out/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me1cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0me2docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_transcripts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exp2_trans_out/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26fd8a9c360a>\u001b[0m in \u001b[0;36mload_transcripts\u001b[0;34m(folder, conds)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtext_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtype_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_token_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mmtld_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_ready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26fd8a9c360a>\u001b[0m in \u001b[0;36mtype_token_count\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#print(lemmatized_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#type_value = len(set(str.split(lemmatized_output)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtype_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatized_output' is not defined"
     ]
    }
   ],
   "source": [
    "e1docs = load_transcripts(\"exp1_trans_out/\", e1cond)\n",
    "e2docs = load_transcripts(\"exp2_trans_out/\", e2cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1docs[['sid','age','AA','EL','RR','video','gender','parent_ed','condition','types','types2','tokens','tokens2','TTR','TTR2','MTLD','MTLD2']].to_csv('exp1_conditions2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2docs[['sid','age','video','gender','parent_ed','condition','types','types2','tokens','tokens2','TTR','TTR2','MTLD','MTLD2']].to_csv('exp2_conditions2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for fname in files\n",
    "\n",
    "def doc_to_df(doc):\n",
    "    cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
    "    rows = []\n",
    "    sdoc = nlp(doc)\n",
    "    for t in sdoc:\n",
    "        row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utterance_count_and_length(doc, verbose=True):\n",
    "    if verbose:\n",
    "        for sent in docs[fname].sents:\n",
    "            print(\">\", sent)\n",
    "    return((len(doc.sents), len(doc) / len(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for chunk in doc.noun_chunks:\n",
    "#    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entities\n",
    "#for ent in docs[fname].ents:\n",
    "#    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "#displacy.render(docs[fname], style=\"ent\") # display dependencies\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# iterate over docs[fname] and concatenate all\n",
    "\n",
    "def count_tokens_nouns_verbs(doc):\n",
    "    # all tokens, excluding stop words and punctuation\n",
    "    words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    nouns = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"NOUN\"]\n",
    "    verbs = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"VERB\"]\n",
    "    return (Counter(words), Counter(nouns), Counter(verbs))\n",
    "    \n",
    "#word_freq, noun_freq, verb_freq = count_tokens_nouns_verbs(combined_)\n",
    "#noun_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "\n",
    "# should I do these merges or not? (maybe not for finding single tokens..)\n",
    "#if \"merge_entities\" not in nlp.pipe_names:\n",
    "#    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "\n",
    "#if \"merge_noun_chunks\" not in nlp.pipe_names:\n",
    "#    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
    "\n",
    "\n",
    "e1corpus = st.CorpusFromPandas(e1docs, category_col=\"condition\", text_col=\"text\", nlp=nlp).build()\n",
    "\n",
    "e2corpus = st.CorpusFromPandas(e2docs, category_col=\"condition\", text_col=\"text\", nlp=nlp).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def print_indicative_terms(corpus):    \n",
    "    term_freq = corpus.get_term_freq_df()\n",
    "    term_freq['Activity Video Score'] = corpus.get_scaled_f_scores('exp')\n",
    "    pprint(list(term_freq.sort_values(by='Activity Video Score', ascending=False).index[:10]))\n",
    "    term_freq['Control Score'] = corpus.get_scaled_f_scores('con')\n",
    "    pprint(list(term_freq.sort_values(by='Control Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 indicative words for activity video vs. control conditions:\n",
      "['give me',\n",
      " 'me the',\n",
      " 'give',\n",
      " 'ribbit ribbit',\n",
      " 'big',\n",
      " 'small',\n",
      " 'ribbit',\n",
      " 'school bus',\n",
      " 'the big',\n",
      " 'put the']\n",
      "['vroom',\n",
      " 'shake shake',\n",
      " 'shake',\n",
      " 'what ’s',\n",
      " 'ready',\n",
      " '’s this',\n",
      " 'it ’s',\n",
      " 'are you',\n",
      " 'oh',\n",
      " 'what is']\n",
      "Experiment 2 indicative words for activity video vs. control conditions:\n",
      "['you give',\n",
      " 'big car',\n",
      " 'small',\n",
      " 'the little',\n",
      " 'give',\n",
      " 'the big',\n",
      " 'you put',\n",
      " 'big',\n",
      " 'cow',\n",
      " 'put the']\n",
      "['beep',\n",
      " 'tap',\n",
      " 'neigh',\n",
      " 'is that',\n",
      " 'like',\n",
      " 'for',\n",
      " 'uhoh',\n",
      " 'say',\n",
      " 'you like',\n",
      " 'does']\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment 1 indicative words for activity video vs. control conditions:\")\n",
    "print_indicative_terms(e1corpus)\n",
    "print(\"Experiment 2 indicative words for activity video vs. control conditions:\")\n",
    "print_indicative_terms(e2corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1html = st.produce_scattertext_explorer(\n",
    "    e1corpus,\n",
    "    category=\"exp\",\n",
    "    category_name=\"Activity Video\",\n",
    "    not_category_name=\"No Video\",\n",
    "    width_in_pixels=1000,\n",
    "    metadata=e1docs[\"sid\"] # need this? other var? (age?)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2html = st.produce_scattertext_explorer(\n",
    "    e2corpus,\n",
    "    category=\"exp\",\n",
    "    category_name=\"Activity Video\",\n",
    "    not_category_name=\"Science Video\",\n",
    "    width_in_pixels=1000,\n",
    "    metadata=e2docs[\"sid\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"Exp1_relative_frequency.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a5faaf8d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "file_name = \"Exp1_relative_frequency.html\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(e1html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"700\"\n",
       "            src=\"Exp2_relative_frequency.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a5faafb70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "file_name = \"Exp2_relative_frequency.html\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(e2html.encode(\"utf-8\"))\n",
    "\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450900"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = (e1corpus\n",
    "          .get_unigram_corpus()\n",
    "          .compact(st.ClassPercentageCompactor(term_count=2,\n",
    "                                               term_ranker=st.OncePerDocFrequencyRanker)))\n",
    "html = st.produce_characteristic_explorer(\n",
    "\tcorpus,\n",
    "\tcategory='exp',\n",
    "\tcategory_name='Activity Video',\n",
    "\tnot_category_name='No Video',\n",
    "\tmetadata=corpus.get_df()['sid']\n",
    ")\n",
    "open('Exp1_characteristic_chart.html', 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514971"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = (e2corpus\n",
    "          .get_unigram_corpus()\n",
    "          .compact(st.ClassPercentageCompactor(term_count=2,\n",
    "                                               term_ranker=st.OncePerDocFrequencyRanker)))\n",
    "html = st.produce_characteristic_explorer(\n",
    "\tcorpus,\n",
    "\tcategory='exp',\n",
    "\tcategory_name='Activity Video',\n",
    "\tnot_category_name='Science Video',\n",
    "\tmetadata=corpus.get_df()['sid']\n",
    ")\n",
    "open('Exp2_characteristic_chart.html', 'wb').write(html.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
