---
title: "Understanding the impacts of video-guided activities on parent-child interaction"
author-information: "\\author{ \n  {\\large\\bf George~Kachergis}^1 (\\texttt{kachergis@stanford.edu})
  \\\\\n  {\\large\\bf Emily~Hembacher}^2 (\\texttt{emily.hembacher@gmail.com}) \\\\\n
  \ {\\large\\bf Veronica~Cristiano}^2 (\\texttt{CRISTIANO @gallaudet.edu}) \\\\\n
  \ {\\large\\bf Hanwen Vivian~Zhang}^1 (\\texttt{vivian3@stanford.edu}) \\\\\n  {\\large\\bf
  Michael C.~Frank}^1 (\\texttt{mcfrank@stanford.edu}) \\\\ \n^{1}Department of Psychology,
  Stanford University \\\\  Stanford, CA 94305 USA \\\\ ^{2} Nextdoor, Inc., Burlingame,
  CA \\\\ ^{3} Gallaudet University, Washington, D.C. }\n"
header-includes:
   - \usepackage{float}
bibliography: library.bib
output:
  pdf_document: default
  html_document:
    df_print: paged
document-params: 10pt, letterpaper
final-submission: \cogscifinalcopy
keywords: "digital parenting advice; joint attention; lexical diversity; guided play;
  \n"
csl: apa6.csl
abstract: |
  Early parenting practices play an important role in shaping the future  outcomes of young children [@Hart1995; @Heckman2006].  In particular, high-quality early interactions and language input appear to  facilitate language learning and result in higher levels of school performance.  The rise of phone- and tablet-based parenting applications ("apps") holds the promise of delivering  low-cost, positive interventions on parenting style to a wide variety of populations.  Of special interest are the parents of very young children, who are often difficult to reach in other ways.  Yet little is known about the effects of communicating to parents through app-based interventions. In a study of one commercial app offering a collection of age-appropriate activity videos, we find  that the quality of parent-child interactions increases in some ways as a result of using the app.  Specifically, the lexical diversity of parents' child-directed speech increases, and measures of  joint attention show...
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r libraries, include=F}
suppressPackageStartupMessages(c("dplyr","langcog","tidyr","ggplot2","lme4"))
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(psych)
library(langcog)
library(tidyverse)
library(ggthemes)
library(lme4)
library(lmerTest)
source("multiplot.R")

select <- dplyr::select # masked by MASS
theme_set(theme_few())
```

# Introduction

The quantity and quality of early language input has been found to be strongly associated with later language and academic outcomes [@Hart1995; @Marchman2008; @Cartmill2013; @HirshPasek2015]. Thus, because of the potential for large downstream effects [@Heckman2006], there is tremendous interest in interventions that change children's language environment. 
And because parents define a large portion of that environment, especially before the onset of formal schooling, parent behavior is a critical locus for such interventions. 
Many effective parenting interventions require large resource investments and require many hours of in-person contact [@Jamaica2014; @PerryPreschool2004], making implementation at scale a daunting proposition. 
For this reason, many researchers targeting early language are interested in delivering parenting interventions remotely -- through texts, apps, and videos delivered on digital devices. 
But what do parents take away from these short messages about what to do or how to talk with their children?

The content provided by digital parenting interventions runs the gamut from general parenting messages and facts from child development research to specific advice and suggested activities. 
A growing body of evidence suggests that these digital interventions can be effective across a range of cultures, income levels, and children's ages [for a review, see @Breitenstein2014].
For example, in contrast to a face-to-face parent training intervention, a tablet-based version saw significantly higher session completion rates (51% attendance vs. 85% module completion) and comparable or larger effect sizes on parents' and children's (aged 2 to 5 years) behavior [@Breitenstein2016].
Often, however, the theory of change presupposed by such interventions is relatively vague.
Both within and outside the realm of academic interventions, messages to parents of young children often seek to provide knowledge about some aspect of development (e.g., early language), often in tandem with a suggestion regarding activities.
Such messages are assumed to inform parents' choice of behaviors, spurring them to engage in some target activity, which is assumed to be more stimulating than what parents would have done otherwise. 

This theory of change is typically grounded in ideas about guided play and early language stimulation. 
Child-directed speech varies not only in quantity (i.e., the number of total tokens), but also in quality in terms of the diversity of the tokens [@Malvern2004] or the context-appropriateness of the speech [@Cartmill2013], both of which have been linked to children's subsequent language development.
Further, language learning -- especially the acquisition of early vocabulary in the first years -- appears to be supported preferentially by parents and children _jointly attending_ to some object or activity [@Baldwin1991; @Bigelow2004].
Episodes of joint attention are frequent during guided play, when parents set goals and scaffold their child's activities [@Wood1976; @Weisberg2013].
Thus, the current literature supports interventions that encourage parents to provide high-quality language and interaction through something like guided play -- whether via reading books or playing with a shape-sorter at home, or via a conversation about categories in the supermarket.

But is this theory of change correct? That is, does the provision of knowledge and activities lead to higher-quality play? 
Alternatively, this theory could be wrong in a number of ways. 
By focusing parents on a specific activity, parents may over-focus on achieving the superficial goals of the activity. 
This problem might be especially likely with video messages, which could encourage parents to try to mimic a model's specific speech and/or actions.
Attempting to reproduce such surface details of a video-guided activity could in turn result in less high-quality talk, with less responsiveness to their child's play.
Another possibility is that these messages do produce the desired effect, but only for those parents who already have a general orientation towards children's early learning.

Our current experiments were designed to make a direct test of this question: How do parents change their interactions with young children on the basis of short video parenting messages? 
In two experiments, we collected data from parent-child dyads in a local children's museum. 
We showed parents in the experimental group a single short video modeling an interactive toy-based activity along with a scientific justification. 
Parents in the control group received either no video (Experiment 1) or a video of a recent finding in developmental psychology (Experiment 2). 
We then gave the toys from the video to all dyads and videotaped their interactions, coding for language quantity and quality as well as joint attention. 


# Experiment 1

In Experiment 1, we invited parents of 6- to 24-month-old infants visiting the Children's Discovery Museum in San Jose to complete video-guided activities from Kinedu (Kinedu Inc.), a parenting app that delivers digital parenting advice in the form of short videos.
Parents were randomly assigned to the video group or the control group; parents in the video group watched a video from the Kinedu app (matched to their child's age), and then performed the activity with their child using the props from the video. 
Parents in the control group did not watch a Kinedu video, but were given a set of the same age-appropriate props and asked to play with their infants as they would at home. 

## Method

```{r, include=F}
e1path = "../../parenting_obs_e1/"
load(paste(e1path,"Exp1_cleaned_data.RData",sep='')) # merged dataframe ready for plotting

lmer_data <- d %>%
  filter(!is.na(AA), !is.na(EL), !is.na(RR))%>%
  mutate(condition = factor(condition), 
         MTLD = as.numeric(MTLD),
         EL = as.numeric(langcog::scale(EL, scale=FALSE)),
         AA = as.numeric(langcog::scale(AA, scale=FALSE)),
         RR = as.numeric(langcog::scale(RR, scale=FALSE)),
         age = as.numeric(langcog::scale(age, scale=FALSE)),
         gender = as.factor(gender),
         video = as.factor(video))

a <- d$gender == "F"
length(a[a == TRUE])


```

### Participants. 
`r nrow(d)` infants (F = 42, M = 18) aged 6-24 months (20 6-11.9 month-olds, 20 12-17.9 month-olds, and 20 18-24 month-olds) and their parents participated in a museum in northern California. 
We included infants who were exposed to English at least 50 percent of the time (n = 58) or who were exposed less but whose participating parent reported that they primarily speak English with their child at home (n = 2). 
61% of participants (n = 37) had been exposed to two or more languages as indicated by their parent. 
Parents identified their children as White (n = 25), Asian (n = 11), African American/Black (n = 2), Biracial (n = 12), other (n = 5), or declined to state (n = 5). 
Fifteen parents reported that their child was of Hispanic origin. 
Parents tended to be highly-educated, with reports of highest level of education ranging from completed high school (n = 5), some college (n = 6), four-year college (n = 14), some graduate school (n = 1), to completed graduate school (n = 28) or declined to state (n = 6).

### Materials.
Stimuli included videos from the Kinedu commercial parenting application. 
The videos were designed to show activities to parents that they could perform with their child in order to foster cognitive and physical development, and were targeted to the child's age and level of development. 
In each video, an adult and child perform the activity while a narrator explains the activity and its purpose. 
We selected two videos for each of three age groups in our sample (6-11.9 months, 12-17.9 months, 18-23.94 months). 
Participants were also given a set of toys corresponding to those in the video that they watched so that they could complete the activity.[^1]

[^1]: Details of the specific videos used and the toys associated with each video are in the Appendix. 

Participants were randomly assigned to either the *Video* condition or the *Control* condition. 
Parents participating in the Video condition were assigned to watch one of the two activity videos available for their child's age group, while parents in the Control condition watched no activity video, and were simply asked to play with their child as they normally would.
The Control condition was yoked to the Activity Video condition such that for every participant in the Video condition who saw a particular video and received the associated props, a participant in the Control condition received the same props but did not watch the activity video.
Parents also completed the Early Parenting Attitudes Questionnaire [EPAQ; @Hembacher2018]. 
The EPAQ measures parents' attitudes about parenting and child development along three dimensions: rules and respect, early learning, and affection and attachment.

### Procedure.
After providing informed consent, parents in the Video condition watched the assigned activity video on a laptop with headphones.
To ensure that parents could give the video their full attention, the experimenter played with the infant with a set of toys (different from the experimental props used in the study) while the video was being played. 
Immediately following the video, each parent-child dyad was provided with the props to complete the video-guided activity that the parent had viewed. 
The toys were placed on a large foam play mat, and parents were instructed to sit on the mat with their child and re-create the activity they had viewed for a period of three minutes.
In the Control condition, after informed consent parents were told to play with their child as they would at home with the provided props for a period of three minutes. 
They were not given any additional instructions about how to use the props.

In both conditions, two video cameras were used to record the play session from different angles, and parents were fitted with a wireless Shurre lavalier microphone to record their child-directed speech. 
After three minutes of play had elapsed, parents were told they could stop playing and the cameras and microphone were turned off.
Parents were then asked to complete the EPAQ before being debriefed. 

### Joint Attention Coding Procedure.
The video of each session was manually coded for episodes of joint attention (JA) using the Datavyu software [@datavyu]. 
The video taken at floor level was coded by default, but the other video was referred to if the participants were not visible or if there was technical difficulty with the first camera. 
Each session's video was coded for episodes of coordinated JA, episodes of passive JA, and parental bids for JA. 
Parental bids for JA were defined as any attempt to initiate joint attention (i.e labeling, pointing, or otherwise drawing attention to an object) that did not result in passive or coordinated JA. 
If more than 3 seconds elapsed between bids, they were coded as separate attempts. 
An episode of joint attention was considered _passive_ if both participants visually focused on an object for 3 or more seconds but the child did not acknowledge the parent. 
If either participant looked away from the object for less than 3 seconds and then returned to the same object it was considered part of the same episode of joint attention. 
A joint attention episode was considered _coordinated_ if both participants visually focused on an object for 3 or more seconds and at some point in the interaction the child indicated awareness of interaction with some overt behavior toward the parent such as looking at their face, gesturing, vocalizing, or turn-taking. 
Full details of our guidelines for coding joint attention are available [here](https://docs.google.com/document/d/1cZrT_Gjt6p0om19lXO-XcFnT09pb0giyhRQ-Pm1yoBU/edit?usp=sharing).

A second coder independently coded a third of the videos (i.e., 20 of the 60 videos, approximately equally distributed across ages) to establish reliability. 
The two coders had a reliability of ICC = 0.80 with 95% confident interval (CI) = [0.57,0.92] for number of parent bids for JA; ICC = 0.20 with 95% CI = [-0.26,0.58] for number of passive JA episodes; ICC = 0.66 with 95% CI = [0.32,0.85] for number of coordinated JA episodes; ICC = 0.24 with 95% CI = [-0.21,0.61] for total duration of passive JA episodes, and ICC = 0.62 with 95% CI = [0.27,0.83] for total duration of coordinated JA episodes. 

## Results

Parents' child-directed speech during the play sessions was transcribed.
The transcripts and hand-coded joint attention data were analyzed according to our preregistration[^2], with any deviations or extensions noted.
Below we first report the lexical diversity results, followed by the joint attention results.

[^2]: Preregistration: [https://osf.io/2bpdf/](https://osf.io/2bpdf/)]

### Lexical Diversity

For each transcript, the words were lemmatized using ``spacy2`` [@spacy2], and the word *types* (unique words) and *tokens* (total words) were then tallied and the type-token ratio (TTR) calculated as a measure of lexical diversity. 
Although TTR was our preregistered measure of lexical diversity as it has commonly been used, it has been noted that TTR is correlated with the length of a text, which has led to the development of new measures such as the measure of textual lexical diversity [MTLD; @McCarthy2010]. 
Thus, we also measure lexical diversity with MTLD, which is calculated as the mean length of sequential word strings in a text that maintain a given TTR value (here we use the value proposed by @McCarthy2010: 0.720).


```{r, e1-lexdiv-regressions, echo=F, include=F}
# Predicting lexical diversity based on experimental condition, PAQ, demographics. 
modTTR <- lmer(TTR ~ condition + age + gender + parent_ed + (1|video), data=lmer_data)
summary(modTTR) # condition -0.123 t(39.9) = 4.25 p<.001

modMTLD <- lmer(MTLD ~ condition + age + gender + parent_ed + (1|video), data=lmer_data)
summary(modMTLD) # condition -11.67 t(43) = 3.08 p=.004 !! singular fit
# Bayesian regression (stan_glmer in rstanarm) shows the same

# predict word tokens
tokens_mod <- lmer(tokens ~ condition*EL + condition*AA + condition*RR + age + gender + parent_ed + (1|video),  data=lmer_data)
summary(tokens_mod) # condition 57.23 t(34.4) = 2.19 p<.05

# predict word types
types_mod <- lmer(types ~ condition*EL + condition*AA + condition*RR  + age + gender + parent_ed + (1|video), data=lmer_data) # !! singular fit
summary(types_mod) # nothing
```

We fit a mixed-effects linear regression predicting TTR as a function of condition, age (scaled and 0-centered), gender, and parent's education level with a random intercept per video using lme4 [@lme4]. 
There was significantly lower TTR in the Video condition (mean: 0.32) than in the Control condition (mean: 0.43, $\beta=-.12$, t(39.9) = 4.25, *p*<.001). 
There were no significant effects of age, gender, or parent's level of education.
A similar mixed-effects linear regression instead predicting MTLD also found significantly lower lexical diversity in the Video condition (mean: 15.7) than in the Control condition (mean: 21.9, $\beta=-11.67$, t(43) = 3.08, *p*<.01), with no other significant effects.
Figure 1 shows the mean of each lexical diversity measure (TTR and MTLD) by condition.

We also conducted similar regressions predicting the number of word tokens and types, finding only a significant effect of condition on the number of word tokens ($\beta=57.23$, t(34.4)=2.19, *p*<.05), with parents using more words in the Video condition (mean: 225, 95% CI: [197,252]) than in the Control condition (mean: 165, 95% CI: [139,193]).

(Table with mean and SD of tokens, types, and TTR)

```{r e1lex_div, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3.0, set.cap.width=T, num.cols.cap=1, fig.cap = "Mean lexical diversity scores by condition (left: Type/Token ratio, right: MTLD) in Experiment 1. Error bars show bootstrapped 95 percent confidence intervals (CIs)."}
d$Condition = factor(d$condition, levels = c("con", "exp"), labels = c("Control", "Video"))

# all at once, but there are NAs in SD cols ?
e1 <- d %>% group_by(Condition) %>%
  summarise(TTR=mean(TTR), TTRsd=sd(TTR),
            MTLD=mean(MTLD), MTLDsd=sd(MTLD),
            types=mean(types), types.sd=sd(types),
            tokens = mean(tokens), tokens.sd=sd(tokens))

ms_lex <- d %>%
  group_by(Condition) %>%
  multi_boot_standard(col="TTR") # Type/Token Ratio

ttr_means <- d%>%
  group_by(Condition)%>%
  summarise(mean = mean(TTR), sd = sd(TTR))

# report table with ttr_means lex_sds

e1ttr <- ggplot(ms_lex, aes(x=Condition, y=mean, fill=Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  xlab("Condition") + ylab("Type/Token Ratio (TTR)") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

ms_lex_mtld <- d %>%
  group_by(Condition) %>%
  multi_boot_standard(col="MTLD") 

mtld_means <- d %>%
  group_by(Condition)%>%
  summarise(mean = mean(MTLD), sd=sd(MTLD))

# report table with mtld_means

e1mtld <- ggplot(ms_lex_mtld, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("MTLD") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") 

multiplot(e1ttr, e1mtld, cols=2)
```

Word tokens and word types

```{r e1token_type, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3.0, set.cap.width=T, num.cols.cap=1, fig.cap = "Mean number of word types and word tokens by condition in Experiment 1."}
ms_tok <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "tokens") 

e1tokens <- ggplot(ms_tok, aes(x=Condition, y = mean, fill = Condition)) +geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Total Number of Word Tokens") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

ms_type <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "types") 

e1types <- ggplot(ms_type, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
             position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Total Number of Word Types") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

multiplot(e1tokens, e1types, cols=2)
```

```{r, e1-effect-size, echo=F}
cohens_d <- diff(ttr_means$mean) / (sqrt(sum(ttr_means$sd^2)) / 2)
```

### Joint Attention

```{r, e1ja-regressions, echo=F, include=F}
load(paste(e1path,"joint_attention/Exp1_joint_attention_data.RData",sep=''))
d$Condition <- factor(d$Condition, levels = c("con","exp"), labels = c("Control","Video"))

lmer_data <- d %>%
  filter(!is.na(AA), !is.na(EL), !is.na(RR))%>%
  mutate(Condition = factor(Condition), 
         bids_tot = as.numeric(bids_tot),
         EL = as.numeric(langcog::scale(EL, scale=FALSE)),
         AA = as.numeric(langcog::scale(AA, scale=FALSE)),
         RR = as.numeric(langcog::scale(RR, scale=FALSE)),
         age = as.numeric(langcog::scale(age, scale=FALSE)),
         gender = as.factor(gender),
         Video = as.factor(Video))

model_string =  "~ Condition * EL + Condition * AA + Condition * RR  + age + gender + parent_ed + (1| Video)"

# Total number of bids
bids_mod <- lmer(paste("bids_tot", model_string), data = lmer_data)
summary(bids_mod) # condition beta = 3.51, t(40.3) = 2.95 p<.01, gender p=.05 ?

bid_cond <- lmer_data %>% group_by(Condition) %>% 
  summarise(mean = mean(bids), sd = sd(bids)) 
print(bid_cond)

# Episodes of coordinated joint attention.
cja_mod <- lmer(paste("cja", model_string), data = lmer_data)
summary(cja_mod) # NA

# Episodes of passive joint attention.
pja_mod <- lmer(paste("pja", model_string), data = lmer_data)
summary(pja_mod) # marginal RR, beta=-1.07 p=.07, condition*RR beta=1.83 p<.05


# Total duration of passive joint attention.
pja_time_mod <- lmer(paste("pja_length", model_string), data = lmer_data)
summary(pja_time_mod) # NA

# Total duration of coordinated joint attention.
cja_time_mod <- lmer(paste("cja_length", model_string), data = lmer_data)
summary(cja_time_mod) # marginal parent education p=.09
```

We fit a mixed-effects linear regression predicting the number of bids for joint attention (JA) as a function of fixed effects of condition, age (scaled and 0-centered), gender, parent's education level, and the subscales of the EPAQ: Early Learning (EL), Affection and Attachment (AA), and Rules and Respect (RR), along with interactions of condition and EL, AA, and RR. This lme4 model included random intercepts per video. 
There were significantly more bids for JA in the Video condition (mean: 6.24, sd: 2.79) than in the Control condition (mean: 3.56, sd: 2.50, $\beta=3.51$, t(40.3) = 2.95, *p*<.01). 
There were no other significant effects.
Mixed-effects regressions with the same structure were performed predicting the number of episodes of coordinated and passive JA, and the total duration of time spent in coordinated and passive JA.
There were no significant effects on the number or total duration of coordinated JA episodes, nor on the total duration of passive JA episodes. 
For the regression predicting the number of passive JA episodes, the only significant effect was an interaction of condition and RR ($\beta=1.83$, t(41.5) = 2.22, *p*<.05), showing that for parents in the Video condition, those with higher Rules and Respect subscores engaged in more passive JA episodes.
Figure X shows ... by condition.


```{r e1ja-graphs, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3.0, set.cap.width=T, num.cols.cap=1, fig.cap = "Mean number of bids and episodes of Joint Attention in Experiment 1."}

ms_bids <- d %>% group_by(Condition) %>%
  multi_boot_standard(col="bids_tot") 

e1bids <- ggplot(ms_bids, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  xlab("Condition") + ylab("Total Bids for Joint Attention") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") 

# total joint attention episodes
ms_tja <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "total_ja") 

e1tja <- ggplot(ms_tja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Total Episodes of Joint Attention") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") 
  
multiplot(e1bids, e1tja, cols=2)
```

```{r e1ja-graphs-pass-coord, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3.0, set.cap.width=T, num.cols.cap=1, fig.cap = "Average number of passive and coordinated episodes of JA in Experiment 1."}
ms_pja <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "pja") 

e1pja <- ggplot(ms_pja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Episodes of Passive Joint Attention")  +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + theme(legend.position="none") 

#Episodes of coordinated joint attention
ms_cja <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "cja") 

e1cja <- ggplot(ms_cja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Episodes of Coordinated Joint Attention")  +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + theme(legend.position="none") 

multiplot(e1pja, e1cja, cols=2)
```

## Discussion

In summary, while parents produced more word types and tokens after viewing the activity video, lexical diversity (Type/Token ratio and MTLD) was higher when parents were just asked to play as they normally would.
This may suggest that parents in the Video condition are being more repetitive in their attempt to stick to the task prescribed in the video.
Demographics and EPAQ do not interact with condition, but there is a marginal effect of RR score on lexical diversity (lower diversity for higher RR scores), and marginal effects of parent education on word types and tokens (more types and tokens for higher parent education).

There was a main effect of condition on total bids for joint attention. 
Parents in the Video condition, after seeing a video demonstrating an activity, made a greater number of bids for joint attention with their child.
There was no effect of condition on the number of episodes of either passive or coordinated joint attention, or the duration of these episodes. 
There was a marginal effect of gender on bids for joint attention, with parents of males producing more bids. 
There was a marginal interaction between RR scores and condition on passive joint attention, such that the experimental condition increased the number of episodes of PJA to a greater extent for people with high RR scores.
While the electronically-delivered parenting advice increased the number of bids for joint attention by parents, it did not significantly effect the number or duration of episodes of joint attention. 
It may be that child variables had a larger relative impact on the attainment of joint attention.


# Experiment 2

Experiment 1 found that parents who watched a Kinedu video spoke more words overall, but had lower lexical diversity compared to parents who played with their children as they normally would at home. 
Parents who watched a Kinedu video also made more bids for joint attention, although these bids did not result in more episodes of joint attention compared to the control group. 
Experiment 2 attempts to replicate these findings from with a restricted number of preregistered predictions ([link](https://osf.io/2bpdf/)). 

## Method

```{r, echo=F}
e2path = "../../parenting_obs_e2/"
load(paste(e2path,"Exp2_cleaned_data.RData",sep=''))

lmer_data <- d %>%
  mutate(condition = factor(condition), 
         MTLD = as.numeric(MTLD),
         TTR = as.numeric(TTR),
         age = as.numeric(langcog::scale(age, scale=FALSE)),
         video = as.factor(video))
```

### Participants. 
`r nrow(d)` infants (F = 37, M = 47) aged 12-24 months (42 12-17.9 month-olds, 42 18-24 month-olds) and their parents participated in the same museum as Experiment 1. 
We included infants who were exposed to English at least 75 percent of the time or who were exposed less but whose participating parent reported that they primarily speak English with their child at home. 
Forty nine% of participants (n = 41) had been exposed to two or more languages as indicated by their parent. 
Parents identified their children as White (n = 39), Asian (n = 20), African American/Black (n = 1), Biracial (n = 9), other (n = 7), or declined to state (n = 8). Sixteen parents reported their child was of Hispanic origin. 
Parents tended to be highly- educated, with reports of highest level of education ranging from completed high school (n = 0), some college (n = 5), four-year college (n = 28), some graduate school (n = 2), to completed graduate school (n = 35) or declined to state (n = 14).

### Materials. 

The design of Experiment 2 was similar to that of Experiment 1, except that instead of a No-Video control condition, parents instead watched a video that was generally related to child development research, but did not give any specific instructions about how to interact with infants or children. 
This was to control for the possibility that differences in language output and joint attention in Experiment 1 could be due to simply cuing parents to think about infants' learning and cognitive development. 
The videos presented in the Control Video condition were media clips (available on YouTube) of developmental psychologists explaining their research interleaved with footage of infants or toddlers engaged in developmental research studies. 
Thus, the content of the videos superficially matched those in the Activity Video condition, but did not suggest any particular activities.
The videos were trimmed to approximately match the average video length in the Activity Video condition (close to 90 s).

### Procedure. 
The procedure for Experiment 2 matched that of Experiment 1, except that parents in the Control Video condition watched a control video before the play session. 
Consistent with the No-Video control condition in Experiment 1, parents in the Control Video condition were told to play with their child as they would at home, and were not given additional instructions.
The coding procedure also matched that of Experiment 1. A second coder independently coded a third of the videos (i.e., 26 of the 84 videos, approximately equally distributed across ages) to establish reliability. 
The two coders had a reliability of ICC = 0.80 with 95% confident interval (CI) = [0.60,0.90] (p < 0.05) for number of parent bids for JA; ICC = 0.74 with 95% CI = [0.59,0.87] (p < 0.05) for number of passive JA episodes; ICC = 0.78 with 95% CI = [0.58,0.90] (p < 0.05) for number of coordinated JA episodes; ICC = 0.72 with 95% CI = [0.46,0.86] (p < 0.05) for total duration of passive JA episodes, and ICC = 0.88 with 95% CI = [0.75,0.94] (p < 0.05) for total duration of coordinated JA episodes. 


## Results

Parents' child-directed speech was transcribed and processed according to the same procedure used in Experiment 1.

### Lexical Diversity
```{r, e2-lexdiv-regressions, echo=F, include=F}
# Predicting lexical diversity based on experimental condition. 
modTTR <- lmer(TTR ~ age * condition + (1 | video), data=lmer_data) 
summary(modTTR) # condition beta=-.09  t(8.7) = 3.06 p=.01

modMTLD <- lmer(MTLD ~ age * condition + (1 | video), data=lmer_data)
summary(modMTLD) # NA

# predict word tokens
tokens_mod <- lmer(tokens ~ age * condition + (1 | video), data=lmer_data)
summary(tokens_mod) # NA

# predict word types
types_mod <- lmer(types ~ age * condition + (1 | video), data=lmer_data)
summary(types_mod) # NA
```

We fit a mixed-effects linear regression predicting TTR and MTLD as a function of age (scaled and 0-centered) and condition with an interaction term, and with random intercepts per video using lme4 [@lme4].
There was significantly lower TTR in the Video condition (mean: 0.38) than in the Control condition (mean: 0.47, $\beta=-.09$, t(8.7) = 3.06, *p*=.01).
There was no significant effect of age.
A similar mixed-effects linear regression instead predicting MTLD found no significant effects of age or condition.
Figure X shows the mean of each lexical diversity measure (TTR and MTLD) by condition.
Regressions with the same structure predicting the number of word tokens and types found no significant effects of age or condition.

(Table with mean and SD of tokens, types, TTR, MTLD)

```{r e2lexdiv, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Mean lexical diversity scores by condition (left: Type/Token ratio, right: MTLD) in Experiment 2."}

d$Condition = factor(d$condition, levels = c("con", "exp"), labels = c("Control", "Video"))

# all at once, but there are NAs in SD cols ?
e2 <- d %>% group_by(Condition) %>%
  summarise(TTR=mean(TTR), TTRsd=sd(TTR),
            MTLD=mean(MTLD), MTLDsd=sd(MTLD),
            types=mean(types), types.sd=sd(types),
            tokens = mean(tokens), tokens.sd=sd(tokens))

ms_lex <- d %>% group_by(Condition) %>%
  multi_boot_standard(col="TTR") 
ttr_mean <- d%>% group_by(Condition)%>%
  summarise(mean=mean(TTR), sd=sd(TTR))

ttr_means <- d%>%
  group_by(Condition)%>%
  summarise(mean = mean(TTR), sd = sd(TTR))

# report table with ttr_means lex_sds

e2ttr <- ggplot(ms_lex, aes(x=Condition, y=mean, fill=Condition)) + ylim(0.0, 0.6) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  xlab("Condition") + ylab("TTR") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

ms_lex <- d %>% group_by(Condition) %>%
  multi_boot_standard(col="MTLD") 
mltd_mean <- d %>% group_by(Condition)%>%
  summarise(mean = mean(MTLD), sd=sd(MTLD))

mtld_means <- d %>%
  group_by(Condition)%>%
  summarise(mean = mean(MTLD), sd=sd(MTLD))

e2mtld <- ggplot(ms_lex, aes(x = Condition, y = mean, fill = Condition)) + 
  geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("MTLD") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

multiplot(e2ttr, e2mtld, cols=2)
```



```{r e2token-type, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Mean number of word types and word tokens by condition in Experiment 2."}
ms_tok <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "tokens") 

e2tokens <- ggplot(ms_tok, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Total Number of Word Tokens") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

ms_typ <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "types") 

e2types <- ggplot(ms_typ, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Total Number of Word Types") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() +
  theme(legend.position="none") 

multiplot(e2tokens, e2types, cols=2)
```
```{r, e2-effect-size, echo=F}
cohens_d <- diff(ttr_means$mean) / (sqrt(sum(ttr_means$sd^2)) / 2)
```

### Joint Attention
```{r, e2ja-regressions, echo=F, include=F}
load(paste(e2path,"joint_attention/Exp2_joint_attention_data.RData",sep=''))
d$Condition <- factor(d$condition, levels = c("con","exp"), labels = c("Control","Video"))
e2ja = d

t.test(subset(e2ja, Condition=="Control")$bids, subset(e2ja, Condition=="Video")$bids)

# Total number of bids
bids_mod <- lmer(bids ~ age * Condition + (1 | video), data = e2ja)
summary(bids_mod) # singular fit, covergence code: 0 - no effects

# Episodes of coordinated joint attention.
cja_mod <- lmer(cja ~ age * condition + (1 | video), data = e2ja)
summary(cja_mod) # marginal condition: t(79) = 1.88 beta=-5.85 p=.06, age*condition beta=4.92 t(79) = 2.35 p=.02
# also singular fit, covergence code: 0

# Episodes of passive joint attention.
pja_mod <- lmer(pja ~ age * condition + (1 | video), data = e2ja)
summary(pja_mod) # age beta=-2.32 t(26.6) = 2.23 p<.05

# Total duration of passive joint attention.
pja_time_mod <- lmer(pja_length ~ age * condition + (1 | video), data = e2ja)
summary(pja_time_mod) # age beta=-15512 t(79) = 2.40 p<.05

# Total duration of coordinated joint attention.
cja_time_mod <- lmer(cja_length ~ age * condition + (1 | video), data = e2ja)
summary(cja_time_mod) # marginal age: beta=31757 t(79) = 1.81 p=.07
```

```{r e2ja-graphs, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3.0, set.cap.width=T, num.cols.cap=1, fig.cap = "Mean number of bids and episodes of Joint Attention in Experiment 2."}

ms_bids <- d %>% group_by(Condition) %>%
  multi_boot_standard(col="bids") 

e2bids <- ggplot(ms_bids, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9)) +
  xlab("Condition") + ylab("Total Bids for Joint Attention") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") 

# total joint attention episodes
ms_tja <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "total_ja") 

e2tja <- ggplot(ms_tja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Total Episodes of Joint Attention") +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + 
  theme(legend.position="none") 
  
multiplot(e2bids, e2tja, cols=2)
```

```{r e2ja-graphs-pass-coord, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.5, fig.height=3.0, set.cap.width=T, num.cols.cap=1, fig.cap = "Average number of passive and coordinated episodes of JA in Experiment 2."}
ms_pja <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "pja") 

e2pja <- ggplot(ms_pja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Episodes of Passive Joint Attention")  +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + theme(legend.position="none") 

#Episodes of coordinated joint attention
ms_cja <- d %>% group_by(Condition) %>%
  multi_boot_standard(col = "cja") 

e2cja <- ggplot(ms_cja, aes(x = Condition, y = mean, fill = Condition)) + geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), position = position_dodge(width = .9))+
  xlab("Condition") + ylab("Episodes of Coordinated Joint Attention")  +
  langcog::scale_colour_solarized() + ggthemes::theme_few() + theme(legend.position="none") 

multiplot(e2pja, e2cja, cols=2)
```

There was a marginal effect of condition on total bids for joint attention. Parents in the experimental condition (i.e., those who saw a video demonstrating an activity) made a greater number of bids for joint attention with their child.
There was no effect of condition on the number of episodes of passive or coordinated JA, nor on the duration of these episodes. 
For passive joint attention, there was a main effect of age on both the number and duration of episodes, with older children having fewer episodes of passive JA and episodes of shorter duration.
Moreover, older children had longer duration episodes of coordinated JA.
These results suggest that as children age, they become more socially engaged in interactions with their caregivers.

# Discussion

In both experiments, the number of tokens was higher in the experimental condition, while the number of types and lexical diversity (Type/Token ratio) were higher in the control condition. 
Parents may be relatively more repetetive in the experimental condition since they are attempting to stick to a specific prescribed task, but they talk more overall.


# Acknowledgements

This work was supported by a gift from Kinedu, Inc. 
Thanks to members of the Language and Cognition Lab at Stanford for helpful discussion.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

# Appendix

## Experiment 1

### Video A (6-11.9 months) "Pick it up"

Parents are told to encourage their child to pick up and drop individual objects. They are also encouraged to place toys on a small cloth and show the child that they can drag the cloth towards them to reach the toys. 

Props: cloth, plastic horse, plastic sheep, plastic elephant, toy car

### Video B (6-11.9 months) “Animal sounds”

Parents are told to call different animals and imitate different sounds the animals make. They are also encourgaed to observe which animal the child prefers.

Props: plastic sheep, plastic horse, plastic frog, plastic cow, bowls

### Video C (12-17.9 months) "Give me the toy"

Parents are told to ask their child to hand over individual toys. They are also encouraged to praise the child after they give them the toys, and repeat the process until the child could follow the verbal instructions.

Props: toy boat, plastic frog, plastic elephant, toy bus

### Video D (12-17.9 months) "Classifying my toys"

Parents are told to place toys of different sizes (big or small) in two hoops. They are also encouraged to ask their child to distinguish between two objects and identify which one is larger.

Props: two yellow and green rings, big car, small car, big horse, small horse

### Video E (18-23.9 months) "My toys"

Parents are told to show the child toys of the same shape but different sizes, to place one of the objects in a basket and to ask the child to take out the object. They are also encouraged to ask their child if the object is bigger or smaller compared to its pair.

Props: two buckets, big car, small car, big horse, small horse

### Video F (18-23.9 months) "The Orchestra"

Parents are told to give their child a musical instrument to play. They are also encouraged to play a song and see if the child follows the rhythm.

Props: maracas, drum, tambourine, clapper

## Experiment 2

### Video A (12-17.9 months) "Give me the toy"

Parents are told to ask their child to hand over individual toys. They are also encouraged to praise the child after they give them the toys, and repeat the process until the child could follow the verbal instructions.

Props: plastic pig, plastic horse, plastic dog, plastic cat, plastic cow

### Video B (12-17.9 months) "Classifying my toys"

Parents are told to place toys of different sizes (big or small) in two hoops. They are also encouraged to ask their child to distinguish between two objects and identify which one is larger.

Props: two yellow and green rings, big car, small car, big horse, small horse

### Video C (12-17.9 months) "Geometric shapes jigzsaw puzzle"

Parents are told to encourage their child to name different shapes on a jigzsaw puzzle. Then they are told to undo the puzzle and invite the child to complete the puzzle. 

Props: A jigzsaw puzzle of geometric shapes

### Video D (18-23.9 months) "My toys"

Parents are told to show the child toys of the same shape but different sizes, to place one of the objects in a basket and to ask the child to take out the object. They are also encouraged to ask their child if the object is bigger or smaller compared to its pair.

Props: two buckets, big car, small car, big horse, small horse

### Video E (18-23.9 months) "The Orchestra"

Parents are told to give their child a musical instrument to play. They are also encouraged to play a song and see if the child follows the rhythm.

Props: maracas, drum, tambourine, clapper

### Video F (18-23.9 months) "My Yellow Toys"

Parents are told to show their child yellow toys and to ask "what color are they". They are also told to give the child toys of different colors, ask them to only play with the yellow ones and praise the child after they do so.

Props: blue car, yellow car, yellow block, red block, blue block, green block

